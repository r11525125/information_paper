#!/usr/bin/env python3
"""
å®Œæ•´ä¿®æ­£çš„KITTI LiDARå£“ç¸®å¯¦é©—
åŒ…å«çœŸå¯¦æ•¸æ“šæ¸¬è©¦ã€TSNç¶²è·¯åˆ†æã€IPFSå„²å­˜è©•ä¼°
Complete corrected experiment with real KITTI data
"""

import os
import sys
import time
import subprocess
import pandas as pd
import numpy as np
import json
from datetime import datetime
from pathlib import Path

class CompleteKITTIExperiment:
    def __init__(self):
        """åˆå§‹åŒ–å¯¦é©—ç’°å¢ƒ"""
        self.base_dir = Path("/home/adlink/å®‡ç¿°è«–æ–‡")
        self.kitti_dir = Path("/home/adlink/ä¸‹è¼‰/KITTI/KITTI")
        self.output_dir = self.base_dir / "outputs"
        self.venv_path = self.base_dir / ".venv"

        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        self.output_dir.mkdir(exist_ok=True)

        # Velodyne HDL-64E è¦æ ¼
        self.lidar_specs = {
            'frame_rate': 10,  # Hz (æ¯ç§’10å€‹frame)
            'frame_period_ms': 100,  # ms (æ¯å€‹frame 100ms)
            'avg_frame_size_mb': 1.88,  # MB (å¹³å‡æ¯å€‹frameå¤§å°)
        }

        # ç¶²è·¯è¦æ ¼
        self.network_specs = {
            'ethernet_100': {'bandwidth_mbps': 100, 'base_latency_ms': 5},
            'tsn_1000': {'bandwidth_mbps': 1000, 'base_latency_ms': 2}
        }

        # å£“ç¸®æ–¹æ³•è™•ç†æ™‚é–“ (ms)
        self.compression_delays = {
            'Huffman': 2.8,
            'EB-HC(Axis)': 3.5,
            'EB-HC(L2)': 4.1,
            'EB-Octree(Axis)': 5.0,
            'EB-Octree(L2)': 5.0,
            'EB-HC-3D(Axis)': 5.2,
            'EB-HC-3D(L2)': 6.1
        }

        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    def verify_kitti_dataset(self):
        """é©—è­‰KITTIæ•¸æ“šé›†å®Œæ•´æ€§"""
        print("\n" + "="*70)
        print("ğŸ“‚ æ­¥é©Ÿ1: é©—è­‰KITTIæ•¸æ“šé›†")
        print("="*70)

        scenes = ['campus', 'city', 'person', 'residential', 'road']
        dataset_info = {}
        total_files = 0
        total_size_gb = 0

        for scene in scenes:
            scene_path = self.kitti_dir / scene
            if not scene_path.exists():
                print(f"âŒ å ´æ™¯ {scene} ä¸å­˜åœ¨!")
                return False

            # çµ±è¨ˆ.binæª”æ¡ˆ
            bin_files = list(scene_path.rglob("*.bin"))
            file_count = len(bin_files)

            # è¨ˆç®—ç¸½å¤§å°
            size_bytes = sum(f.stat().st_size for f in bin_files)
            size_gb = size_bytes / (1024**3)

            dataset_info[scene] = {
                'files': file_count,
                'size_gb': size_gb
            }

            total_files += file_count
            total_size_gb += size_gb

            print(f"âœ“ {scene:12s}: {file_count:4d} æª”æ¡ˆ, {size_gb:.3f} GB")

        print(f"\nç¸½è¨ˆ: {total_files} æª”æ¡ˆ, {total_size_gb:.2f} GB")

        # å„²å­˜æ•¸æ“šé›†è³‡è¨Š
        self.dataset_info = dataset_info
        self.total_files = total_files
        self.total_size_gb = total_size_gb

        return True

    def run_real_compression(self, sample_size=3):
        """åŸ·è¡ŒçœŸå¯¦å£“ç¸®å¯¦é©—"""
        print("\n" + "="*70)
        print("ğŸ”¬ æ­¥é©Ÿ2: åŸ·è¡ŒçœŸå¯¦å£“ç¸®å¯¦é©—")
        print("="*70)

        all_results = []

        for scene, info in self.dataset_info.items():
            print(f"\nè™•ç† {scene} å ´æ™¯ (æ¡æ¨£ {sample_size}/{info['files']} æª”æ¡ˆ)...")

            scene_dir = self.kitti_dir / scene
            output_csv = self.output_dir / f"compression_{scene}_{self.timestamp}.csv"

            # åŸ·è¡Œå£“ç¸®å¯¦é©—
            cmd = [
                "bash", "-c",
                f"source {self.venv_path}/bin/activate && "
                f"python {self.base_dir}/scripts/run_subset_experiments.py "
                f"--data-dir {scene_dir} "
                f"--max-files {sample_size} "
                f"--be-list 1.0 "
                f"--out-csv {output_csv}"
            ]

            try:
                print(f"  åŸ·è¡Œå£“ç¸®æ¸¬è©¦...")
                start = time.time()
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=180)
                elapsed = time.time() - start

                if result.returncode == 0 and output_csv.exists():
                    df = pd.read_csv(output_csv)
                    df['Scene'] = scene
                    all_results.append(df)
                    print(f"  âœ“ æˆåŠŸ: {len(df)} ç­†è¨˜éŒ„ ({elapsed:.1f}ç§’)")
                else:
                    print(f"  âŒ å¤±æ•—: {result.stderr[:200]}")

            except Exception as e:
                print(f"  âŒ éŒ¯èª¤: {e}")

        if all_results:
            # åˆä½µæ‰€æœ‰çµæœ
            self.compression_df = pd.concat(all_results, ignore_index=True)
            output_path = self.output_dir / f"compression_all_{self.timestamp}.csv"
            self.compression_df.to_csv(output_path, index=False)

            print(f"\nâœ“ å£“ç¸®å¯¦é©—å®Œæˆ: {len(self.compression_df)} ç­†è¨˜éŒ„")
            print(f"  å„²å­˜è‡³: {output_path}")

            # è¨ˆç®—å¹³å‡å£“ç¸®ç‡
            avg_ratios = self.compression_df.groupby('Method')['Compression Ratio'].mean()
            print("\nå¹³å‡å£“ç¸®ç‡:")
            for method, ratio in avg_ratios.items():
                print(f"  {method:20s}: {ratio:.4f}")

            return True
        return False

    def analyze_tsn_performance(self):
        """åˆ†æTSNç¶²è·¯æ€§èƒ½"""
        print("\n" + "="*70)
        print("ğŸ“¡ æ­¥é©Ÿ3: TSNç¶²è·¯æ€§èƒ½åˆ†æ")
        print("="*70)

        results = []

        # LiDARæ•¸æ“šåƒæ•¸
        frame_rate = self.lidar_specs['frame_rate']
        frame_size_mb = self.lidar_specs['avg_frame_size_mb']
        frame_period_ms = self.lidar_specs['frame_period_ms']

        # è¨ˆç®—æ¯”ç‰¹ç‡
        bitrate_mbps = frame_size_mb * 8 * frame_rate

        print(f"\nLiDARæ•¸æ“šæµ:")
        print(f"  Frameç‡: {frame_rate} Hz")
        print(f"  Frameå¤§å°: {frame_size_mb} MB")
        print(f"  æ•¸æ“šç‡: {bitrate_mbps:.1f} Mbps")

        # 1. è»Šå…§ä¹™å¤ªç¶²è·¯ (100 Mbps) - æœªå£“ç¸®
        print("\nã€è»Šå…§ä¹™å¤ªç¶²è·¯ 100Mbpsã€‘")
        eth_bw = self.network_specs['ethernet_100']['bandwidth_mbps']
        eth_latency = self.network_specs['ethernet_100']['base_latency_ms']

        eth_util = (bitrate_mbps / eth_bw) * 100
        eth_trans_ms = (frame_size_mb * 8 / eth_bw) * 1000

        if eth_util > 100:
            print(f"  âŒ é »å¯¬ä¸è¶³! éœ€è¦ {bitrate_mbps:.1f} Mbps > {eth_bw} Mbps")
            print(f"  ç¶²è·¯åˆ©ç”¨ç‡: {eth_util:.1f}% (éè¼‰)")
            print(f"  æœƒé€ æˆå°åŒ…ç´¯ç©å’Œä¸Ÿå¤±")
            eth_total_latency = float('inf')  # è¡¨ç¤ºç„¡æ³•è™•ç†
        else:
            eth_queuing = eth_util * 0.15  # æ’éšŠå»¶é²
            eth_total_latency = eth_latency + eth_trans_ms + eth_queuing
            print(f"  ç¶²è·¯åˆ©ç”¨ç‡: {eth_util:.1f}%")
            print(f"  ç¸½å»¶é²: {eth_total_latency:.2f} ms")

        results.append({
            'Network': 'Ethernet_100Mbps',
            'Method': 'Uncompressed',
            'Compression_Ratio': 1.0,
            'Frame_Size_MB': frame_size_mb,
            'Bitrate_Mbps': bitrate_mbps,
            'Network_Utilization_%': eth_util,
            'Transmission_Time_ms': eth_trans_ms,
            'Total_Latency_ms': eth_total_latency,
            'Feasible': eth_util < 90,
            'Meets_Deadline': eth_total_latency < frame_period_ms
        })

        # 2. TSN (1 Gbps) - æœªå£“ç¸®
        print("\nã€TSN 1Gbps - æœªå£“ç¸®ã€‘")
        tsn_bw = self.network_specs['tsn_1000']['bandwidth_mbps']
        tsn_latency = self.network_specs['tsn_1000']['base_latency_ms']

        tsn_util = (bitrate_mbps / tsn_bw) * 100
        tsn_trans_ms = (frame_size_mb * 8 / tsn_bw) * 1000
        tsn_queuing = tsn_util * 0.05  # TSNä½æ’éšŠå»¶é²
        tsn_total_latency = tsn_latency + tsn_trans_ms + tsn_queuing

        print(f"  ç¶²è·¯åˆ©ç”¨ç‡: {tsn_util:.1f}%")
        print(f"  å‚³è¼¸æ™‚é–“: {tsn_trans_ms:.3f} ms")
        print(f"  ç¸½å»¶é²: {tsn_total_latency:.2f} ms")
        print(f"  âœ“ æ»¿è¶³100ms deadline: {tsn_total_latency < frame_period_ms}")

        results.append({
            'Network': 'TSN_1Gbps',
            'Method': 'Uncompressed',
            'Compression_Ratio': 1.0,
            'Frame_Size_MB': frame_size_mb,
            'Bitrate_Mbps': bitrate_mbps,
            'Network_Utilization_%': tsn_util,
            'Transmission_Time_ms': tsn_trans_ms,
            'Total_Latency_ms': tsn_total_latency,
            'Feasible': True,
            'Meets_Deadline': tsn_total_latency < frame_period_ms
        })

        # 3. TSN + å£“ç¸®
        if hasattr(self, 'compression_df'):
            print("\nã€TSN 1Gbps - å£“ç¸®ã€‘")

            avg_ratios = self.compression_df.groupby('Method')['Compression Ratio'].mean()

            for method, ratio in avg_ratios.items():
                if method in self.compression_delays:
                    compressed_size_mb = frame_size_mb * ratio
                    compressed_bitrate = compressed_size_mb * 8 * frame_rate
                    compressed_util = (compressed_bitrate / tsn_bw) * 100
                    compressed_trans_ms = (compressed_size_mb * 8 / tsn_bw) * 1000

                    processing_ms = self.compression_delays[method]
                    compressed_queuing = compressed_util * 0.05
                    compressed_total = tsn_latency + processing_ms + compressed_trans_ms + compressed_queuing

                    print(f"\n  {method}:")
                    print(f"    å£“ç¸®æ¯”: {ratio:.4f}")
                    print(f"    å£“ç¸®è™•ç†: {processing_ms:.1f} ms")
                    print(f"    ç¶²è·¯åˆ©ç”¨ç‡: {compressed_util:.1f}%")
                    print(f"    ç¸½å»¶é²: {compressed_total:.2f} ms")
                    print(f"    æ»¿è¶³deadline: {'âœ“' if compressed_total < frame_period_ms else 'âœ—'}")

                    results.append({
                        'Network': 'TSN_1Gbps',
                        'Method': method,
                        'Compression_Ratio': ratio,
                        'Frame_Size_MB': compressed_size_mb,
                        'Bitrate_Mbps': compressed_bitrate,
                        'Network_Utilization_%': compressed_util,
                        'Processing_Time_ms': processing_ms,
                        'Transmission_Time_ms': compressed_trans_ms,
                        'Total_Latency_ms': compressed_total,
                        'Feasible': True,
                        'Meets_Deadline': compressed_total < frame_period_ms
                    })

        # å„²å­˜çµæœ
        self.tsn_df = pd.DataFrame(results)
        output_path = self.output_dir / f"tsn_analysis_{self.timestamp}.csv"
        self.tsn_df.to_csv(output_path, index=False)

        print(f"\nâœ“ TSNåˆ†æå®Œæˆï¼Œå„²å­˜è‡³: {output_path}")
        return True

    def analyze_ipfs_storage(self):
        """åˆ†æIPFSå„²å­˜ç¯€çœ"""
        print("\n" + "="*70)
        print("ğŸ’¾ æ­¥é©Ÿ4: IPFSå„²å­˜åˆ†æ")
        print("="*70)

        results = []

        # IPFSåƒæ•¸
        upload_speed_mbps = 10  # å…¸å‹ä¸Šå‚³é€Ÿåº¦
        daily_hours = 12  # æ¯å¤©é‹è¡Œæ™‚æ•¸
        days_per_year = 365

        # è¨ˆç®—å¹´åº¦æ•¸æ“šé‡
        frames_per_day = self.lidar_specs['frame_rate'] * 3600 * daily_hours
        frames_per_year = frames_per_day * days_per_year

        print(f"\nå„²å­˜éœ€æ±‚ä¼°ç®—:")
        print(f"  æ¯æ—¥é‹è¡Œ: {daily_hours} å°æ™‚")
        print(f"  æ¯æ—¥frames: {frames_per_day:,}")
        print(f"  å¹´åº¦frames: {frames_per_year:,}")

        # æœªå£“ç¸®å„²å­˜
        uncompressed_gb_per_year = (frames_per_year * self.lidar_specs['avg_frame_size_mb']) / 1024
        uncompressed_upload_hours = (uncompressed_gb_per_year * 1024 * 8) / (upload_speed_mbps * 3600)

        print(f"\nã€æœªå£“ç¸®ã€‘")
        print(f"  å¹´åº¦å„²å­˜: {uncompressed_gb_per_year:.1f} GB")
        print(f"  ä¸Šå‚³æ™‚é–“: {uncompressed_upload_hours:.1f} å°æ™‚")

        results.append({
            'Method': 'Uncompressed',
            'Compression_Ratio': 1.0,
            'Annual_Storage_GB': uncompressed_gb_per_year,
            'Storage_Savings_%': 0,
            'Upload_Time_Hours': uncompressed_upload_hours,
            'Upload_Time_Savings_%': 0
        })

        # å£“ç¸®å„²å­˜
        if hasattr(self, 'compression_df'):
            print("\nã€å£“ç¸®å„²å­˜ç¯€çœã€‘")

            avg_ratios = self.compression_df.groupby('Method')['Compression Ratio'].mean()

            for method, ratio in avg_ratios.items():
                compressed_gb = uncompressed_gb_per_year * ratio
                storage_savings = (1 - ratio) * 100
                compressed_upload_hours = (compressed_gb * 1024 * 8) / (upload_speed_mbps * 3600)
                upload_savings = (1 - ratio) * 100

                print(f"\n  {method}:")
                print(f"    å£“ç¸®æ¯”: {ratio:.4f}")
                print(f"    å¹´åº¦å„²å­˜: {compressed_gb:.1f} GB")
                print(f"    å„²å­˜ç¯€çœ: {storage_savings:.1f}%")
                print(f"    ä¸Šå‚³æ™‚é–“: {compressed_upload_hours:.1f} å°æ™‚")
                print(f"    æ™‚é–“ç¯€çœ: {upload_savings:.1f}%")

                results.append({
                    'Method': method,
                    'Compression_Ratio': ratio,
                    'Annual_Storage_GB': compressed_gb,
                    'Storage_Savings_%': storage_savings,
                    'Upload_Time_Hours': compressed_upload_hours,
                    'Upload_Time_Savings_%': upload_savings
                })

        # å„²å­˜çµæœ
        self.ipfs_df = pd.DataFrame(results)
        output_path = self.output_dir / f"ipfs_analysis_{self.timestamp}.csv"
        self.ipfs_df.to_csv(output_path, index=False)

        print(f"\nâœ“ IPFSåˆ†æå®Œæˆï¼Œå„²å­˜è‡³: {output_path}")
        return True

    def generate_final_report(self):
        """ç”Ÿæˆæœ€çµ‚å¯¦é©—å ±å‘Š"""
        print("\n" + "="*70)
        print("ğŸ“Š æ­¥é©Ÿ5: ç”Ÿæˆæœ€çµ‚å ±å‘Š")
        print("="*70)

        report = {
            'experiment': 'å®Œæ•´KITTI LiDARå£“ç¸®å¯¦é©—',
            'timestamp': self.timestamp,
            'lidar_specs': self.lidar_specs,
            'dataset': {
                'total_files': self.total_files,
                'total_size_gb': round(self.total_size_gb, 2),
                'scenes': list(self.dataset_info.keys())
            }
        }

        # å£“ç¸®çµæœ
        if hasattr(self, 'compression_df'):
            avg_ratios = self.compression_df.groupby('Method')['Compression Ratio'].mean()
            best_method = avg_ratios.idxmin()

            report['compression'] = {
                'records': len(self.compression_df),
                'best_method': best_method,
                'best_ratio': float(avg_ratios[best_method]),
                'methods': {method: float(ratio) for method, ratio in avg_ratios.items()}
            }

        # TSNçµæœ
        if hasattr(self, 'tsn_df'):
            tsn_uncompressed = self.tsn_df[
                (self.tsn_df['Network'] == 'TSN_1Gbps') &
                (self.tsn_df['Method'] == 'Uncompressed')
            ].iloc[0]

            tsn_compressed = self.tsn_df[
                (self.tsn_df['Network'] == 'TSN_1Gbps') &
                (self.tsn_df['Method'] != 'Uncompressed')
            ]

            if not tsn_compressed.empty:
                best_compressed = tsn_compressed.loc[tsn_compressed['Total_Latency_ms'].idxmin()]

                report['tsn'] = {
                    'uncompressed_latency_ms': float(tsn_uncompressed['Total_Latency_ms']),
                    'uncompressed_utilization_%': float(tsn_uncompressed['Network_Utilization_%']),
                    'best_compressed_method': best_compressed['Method'],
                    'best_compressed_latency_ms': float(best_compressed['Total_Latency_ms']),
                    'best_compressed_utilization_%': float(best_compressed['Network_Utilization_%']),
                    'latency_increase_ms': float(best_compressed['Total_Latency_ms'] - tsn_uncompressed['Total_Latency_ms']),
                    'utilization_reduction_%': float(tsn_uncompressed['Network_Utilization_%'] - best_compressed['Network_Utilization_%'])
                }

        # IPFSçµæœ
        if hasattr(self, 'ipfs_df'):
            best_storage = self.ipfs_df.loc[self.ipfs_df['Storage_Savings_%'].idxmax()]

            report['ipfs'] = {
                'best_method': best_storage['Method'],
                'best_storage_savings_%': float(best_storage['Storage_Savings_%']),
                'best_upload_time_savings_%': float(best_storage['Upload_Time_Savings_%']),
                'annual_storage_gb_saved': float(
                    self.ipfs_df[self.ipfs_df['Method'] == 'Uncompressed']['Annual_Storage_GB'].iloc[0] -
                    best_storage['Annual_Storage_GB']
                )
            }

        # å„²å­˜JSONå ±å‘Š
        report_path = self.output_dir / f"final_report_{self.timestamp}.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        # é¡¯ç¤ºé—œéµçµæœ
        print("\n" + "="*70)
        print("ğŸ¯ é—œéµå¯¦é©—çµæœ")
        print("="*70)

        print(f"\nğŸ“‚ æ•¸æ“šé›†:")
        print(f"  â€¢ ç¸½æª”æ¡ˆ: {report['dataset']['total_files']} frames")
        print(f"  â€¢ ç¸½å¤§å°: {report['dataset']['total_size_gb']} GB")

        if 'compression' in report:
            print(f"\nğŸ”¬ å£“ç¸®:")
            print(f"  â€¢ æœ€ä½³æ–¹æ³•: {report['compression']['best_method']}")
            print(f"  â€¢ å£“ç¸®æ¯”: {report['compression']['best_ratio']:.4f}")

        if 'tsn' in report:
            print(f"\nğŸ“¡ TSNç¶²è·¯:")
            print(f"  â€¢ æœªå£“ç¸®å»¶é²: {report['tsn']['uncompressed_latency_ms']:.2f} ms")
            print(f"  â€¢ æœ€ä½³å£“ç¸®å»¶é²: {report['tsn']['best_compressed_latency_ms']:.2f} ms")
            print(f"  â€¢ å»¶é²å¢åŠ : {report['tsn']['latency_increase_ms']:.2f} ms")
            print(f"  â€¢ ç¶²è·¯åˆ©ç”¨ç‡é™ä½: {report['tsn']['utilization_reduction_%']:.1f}%")

        if 'ipfs' in report:
            print(f"\nğŸ’¾ IPFSå„²å­˜:")
            print(f"  â€¢ æœ€ä½³æ–¹æ³•: {report['ipfs']['best_method']}")
            print(f"  â€¢ å„²å­˜ç¯€çœ: {report['ipfs']['best_storage_savings_%']:.1f}%")
            print(f"  â€¢ ä¸Šå‚³æ™‚é–“ç¯€çœ: {report['ipfs']['best_upload_time_savings_%']:.1f}%")

        print(f"\nâœ“ å®Œæ•´å ±å‘Šå·²å„²å­˜: {report_path}")
        return report

    def run(self):
        """åŸ·è¡Œå®Œæ•´å¯¦é©—"""
        print("\n" + "="*80)
        print("ğŸš€ é–‹å§‹å®Œæ•´KITTI LiDARå£“ç¸®å¯¦é©—")
        print("="*80)

        start_time = time.time()

        # æ­¥é©Ÿ1: é©—è­‰æ•¸æ“šé›†
        if not self.verify_kitti_dataset():
            print("âŒ æ•¸æ“šé›†é©—è­‰å¤±æ•—")
            return False

        # æ­¥é©Ÿ2: åŸ·è¡Œå£“ç¸®å¯¦é©—
        if not self.run_real_compression(sample_size=5):  # æ¯å€‹å ´æ™¯æ¸¬è©¦5å€‹æª”æ¡ˆ
            print("âŒ å£“ç¸®å¯¦é©—å¤±æ•—")
            return False

        # æ­¥é©Ÿ3: TSNæ€§èƒ½åˆ†æ
        if not self.analyze_tsn_performance():
            print("âŒ TSNåˆ†æå¤±æ•—")
            return False

        # æ­¥é©Ÿ4: IPFSå„²å­˜åˆ†æ
        if not self.analyze_ipfs_storage():
            print("âŒ IPFSåˆ†æå¤±æ•—")
            return False

        # æ­¥é©Ÿ5: ç”Ÿæˆå ±å‘Š
        report = self.generate_final_report()

        elapsed_time = time.time() - start_time

        print("\n" + "="*80)
        print(f"âœ… å¯¦é©—å®Œæˆï¼ç¸½è€—æ™‚: {elapsed_time:.1f} ç§’")
        print("="*80)

        return report


def main():
    """ä¸»å‡½æ•¸"""
    experiment = CompleteKITTIExperiment()
    report = experiment.run()

    if report:
        print("\nğŸ‰ æ‰€æœ‰å¯¦é©—æˆåŠŸå®Œæˆï¼")
        print("ğŸ“ è¼¸å‡ºæª”æ¡ˆä½æ–¼: outputs/")
        return 0
    else:
        print("\nâŒ å¯¦é©—åŸ·è¡Œå¤±æ•—")
        return 1


if __name__ == "__main__":
    sys.exit(main())